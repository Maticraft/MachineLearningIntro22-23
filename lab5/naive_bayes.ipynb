{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "In this lab, we will implement a Naive Bayes classifier. It is a simple classifier that is often used as a baseline for text classification. It is also a good classifier to use when you have a large number of features.\n",
    "\n",
    "As the name suggests, the Naive Bayes classifier is based on Bayes' theorem. Bayes' theorem is a way to calculate the probability of an event given some prior knowledge. For example, if we know that a person has a cold, we can calculate the probability that they have a fever. The formula for Bayes' theorem is:\n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "In this case, $A$ is the event that the person has a fever, and $B$ is the event that the person has a cold. $P(A)$, called as the **prior** probability, is the probability of the event before we know anything else. In this case, it is the probability that a person has a fever. $P(B)$ is the probability of the **evidence**, or the probability of the cold. $P(A|B)$ is the probability of the event after we know the evidence. In this case, it is the probability that a person has a fever given that they have a cold. $P(B|A)$ is the probability of the evidence given the event. In this case, it is the probability that a person has a cold given that they have a fever.\n",
    "\n",
    "\n",
    "The Naive Bayes classifier is based on Bayes' theorem, but it makes a simplifying \"Naive\" assumption. It assumes that the features are independent of each other. This means that the probability of a feature given the class is the same as the probability of any other feature given the class. For example, if we are classifying emails as spam or not spam, we might use the words \"free\" and \"money\" as features. The Naive Bayes classifier assumes that the probability of the word \"free\" given that the email is spam is the same as the probability of the word \"money\" given that the email is spam. This is a simplifying assumption, but it is often a good one. In the case of the email example, it is unlikely that the word \"free\" would appear in a non-spam email, and it is unlikely that the word \"money\" would appear in a spam email.\n",
    "\n",
    "\n",
    "Putting this all together, the Naive Bayes classifier calculates the probability of a class given the features using Bayes' theorem. It then chooses the class with the highest probability. The formula for the Naive Bayes classifier is:\n",
    "\n",
    "$$P(C_k|F_1, F_2, ..., F_n) = \\frac{P(F_1, F_2, ..., F_n|C_k)P(C_k)}{P(F_1, F_2, ..., F_n)}$$\n",
    "\n",
    "In this case, $C_k$ is the class, and $F_1, F_2, ..., F_n$ are the features. $P(C_k)$ is the prior probability of the class. $P(F_1, F_2, ..., F_n)$ is the probability of the evidence, or the probability of the features. $P(F_1, F_2, ..., F_n|C_k)$ is the probability of the evidence given the class. In this case, it is the probability of the features given the class. $P(C_k|F_1, F_2, ..., F_n)$ is the probability of the class given the features. This is the probability that we are trying to calculate. It is the probability that the class is $C_k$ given the features $F_1, F_2, ..., F_n$.\n",
    "\n",
    "Simplifying the formula, using the chain rule of probability, and assuming that the features are independent, we get:\n",
    "\n",
    "$$P(C_k|F_1, F_2, ..., F_n) = \\frac{P(F_1|C_k)P(F_2|C_k)...P(F_n|C_k)P(C_k)}{P(F_1)P(F_2)...P(F_n)}$$\n",
    "\n",
    "In this case, we are calculating the probability that the class is $C_k$ given the features $F_1, F_2, ..., F_n$. We do this by multiplying the prior probability of the class by the probability of each feature given the class. We then divide by the probability of the evidence. The probability of the evidence is the same for all classes, so we can ignore it when we are choosing the class with the highest probability. Hence, choosing the class with the highest probability is equivalent to choosing the class with the highest prior probability multiplied by the probability of each feature given the class. It can be expressed as:\n",
    "\n",
    "$$\\hat{C} = \\underset{C_k}{\\operatorname{argmax}} P(C_k)P(F_1|C_k)P(F_2|C_k)...P(F_n|C_k)$$\n",
    "\n",
    "In multinomial Naive Bayes, the features are assumed to be counts of the number of times a feature appears in a document. For example, coming back to email spam clasifier, we can count the number of times the words \"free\" and \"money\" appear in spam and non-spam emails. We then divide the number of times the word appears in a class by the total number of words in the class. This gives us the probability of the word given the class. Then, we multiply these probabilities together to get the probability of the class given the features. Finally, we need to calculate the prior probability of the class. This is the probability that an email is spam or not spam. We can calculate this by dividing the number of spam emails by the total number of emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this labs, we will use the [20 Newsgroups dataset](http://qwone.com/~jason/20Newsgroups/). This dataset contains 20,000 newsgroup posts, each labeled with one of 20 topics. The dataset is split into 19,000 training posts and 1,000 test posts. The topics are:\n",
    "\n",
    "```\n",
    "alt.atheism\n",
    "comp.graphics\n",
    "comp.os.ms-windows.misc\n",
    "comp.sys.ibm.pc.hardware\n",
    "comp.sys.mac.hardware\n",
    "comp.windows.x\n",
    "misc.forsale\n",
    "rec.autos\n",
    "rec.motorcycles\n",
    "rec.sport.baseball\n",
    "rec.sport.hockey\n",
    "sci.crypt\n",
    "sci.electronics\n",
    "sci.med\n",
    "sci.space\n",
    "soc.religion.christian\n",
    "talk.politics.guns\n",
    "talk.politics.mideast\n",
    "talk.politics.misc\n",
    "talk.religion.misc\n",
    "```\n",
    "\n",
    "We will use the training set to train a multinomial Naive Bayes classifier, and then use the test set to evaluate the classifier. We will use the [scikit-learn](http://scikit-learn.org/stable/) library to implement the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The data is stored in a directory structure. Each topic is stored in a separate directory. Each post is stored in a separate file. The file name is the post ID. The first line of the file is the post header, which we will ignore. The rest of the file is the post body. The following code loads the data into a list of `(topic, post)` tuples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_data(data_dir):\n",
    "    data = []\n",
    "    for topic in os.listdir(data_dir):\n",
    "        topic_dir = os.path.join(data_dir, topic)\n",
    "        for fname in os.listdir(topic_dir):\n",
    "            with open(os.path.join(topic_dir, fname)) as f:\n",
    "                # Ignore the header\n",
    "                for line in f:\n",
    "                    if not line.strip():\n",
    "                        break\n",
    "\n",
    "                # Read the rest of the post\n",
    "                post = f.read()\n",
    "                data.append((topic, post))\n",
    "    return data\n",
    "\n",
    "train_data = load_data('20news-bydate-train')\n",
    "test_data = load_data('20news-bydate-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Before we can train the classifier, we need to preprocess the data. We will do the following:\n",
    "\n",
    "* Convert all words to lowercase\n",
    "* Remove punctuation\n",
    "* Remove stop words\n",
    "* Stem words\n",
    "\n",
    "The following code defines a function that does all of these preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mati1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def preprocess(text):\n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove words that are one character or less\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([w for w in text.split() if w not in stop_words])\n",
    "\n",
    "    # Stem words\n",
    "    stemmer = PorterStemmer()\n",
    "    text = ' '.join([stemmer.stem(w) for w in text.split()])\n",
    "\n",
    "    return text\n",
    "\n",
    "train_data = [(topic, preprocess(post)) for topic, post in train_data]\n",
    "test_data = [(topic, preprocess(post)) for topic, post in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the vocabulary\n",
    "\n",
    "Now that we have preprocessed the data, we need to create a vocabulary. The vocabulary is a list of all the unique words that appear in the training data. We will use the vocabulary to create a feature vector for each post. The feature vector will contain the frequency of each word in the vocabulary. The following code creates the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(data, max_words = 10000):\n",
    "    # Create a list of all the words in the data\n",
    "    all_words = [word for _, post in data for word in post.split()]\n",
    "\n",
    "    # Create a dictionary mapping each word to its frequency\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Sort the words by frequency in descending order\n",
    "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the words\n",
    "    words = [x[0] for x in sorted_word_counts]\n",
    "\n",
    "    return words[:max_words]\n",
    "\n",
    "\n",
    "vocabulary = create_vocabulary(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the feature vectors\n",
    "\n",
    "Now that we have the vocabulary, we can create the feature vectors. The feature vector for a post is a list of word frequencies. The $i$th element of the feature vector is the frequency of the $i$-th word in the vocabulary. The following code creates the feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = Counter(document.split())\n",
    "    features = [document_words[word] for word in vocabulary]\n",
    "    return features\n",
    "\n",
    "train_features = [(document_features(post), topic) for topic, post in train_data]\n",
    "test_features = [(document_features(post), topic) for topic, post in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the classifier\n",
    "\n",
    "Now that we have the feature vectors, we can train the classifier. We will use the [Naive Bayes classifier](http://scikit-learn.org/stable/modules/naive_bayes.html) from scikit-learn. The following code trains the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(set([topic for topic, _ in train_data]))\n",
    "train_labels = [topics.index(topic) for _, topic in train_features]\n",
    "test_labels = [topics.index(topic) for _, topic in test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = [features for features, _ in train_features]\n",
    "test_samples = [features for features, _ in test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(train_samples, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the classifier\n",
    "\n",
    "Now that we have trained the classifier, we can evaluate it on the test set. The following code evaluates the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9093158918154499\n",
      "Test accuracy: 0.7635422198619225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_predictions = classifier.predict(train_samples)\n",
    "test_predictions = classifier.predict(test_samples)\n",
    "\n",
    "print('Train accuracy:', accuracy_score(train_labels, train_predictions))\n",
    "print('Test accuracy:', accuracy_score(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic                          Accuracy\n",
      "alt.atheism                    0.68\n",
      "soc.religion.christian         0.82\n",
      "comp.sys.ibm.pc.hardware       0.69\n",
      "rec.motorcycles                0.91\n",
      "sci.electronics                0.64\n",
      "talk.religion.misc             0.43\n",
      "rec.sport.baseball             0.89\n",
      "sci.crypt                      0.85\n",
      "sci.med                        0.81\n",
      "sci.space                      0.83\n",
      "talk.politics.guns             0.87\n",
      "talk.politics.misc             0.55\n",
      "rec.sport.hockey               0.94\n",
      "rec.autos                      0.87\n",
      "talk.politics.mideast          0.81\n",
      "comp.os.ms-windows.misc        0.55\n",
      "comp.graphics                  0.73\n",
      "comp.windows.x                 0.69\n",
      "comp.sys.mac.hardware          0.78\n",
      "misc.forsale                   0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print('Topic %-*s Accuracy' % (max([len(topic) for topic in topics]), ''))\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "classes_accuracy = cm.diagonal()/cm.sum(axis=1)\n",
    "for i, accuracy in enumerate(classes_accuracy):\n",
    "    print('%-*s %.2f' % (max([len(topic) for topic in topics]) + 6, topics[i], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science contest #1\n",
    "\n",
    "Build a spam classifier using the SMS Spam Collection dataset from `spam.csv` file. The dataset contains SMS messages, each labeled as \"spam\" or \"ham\" (not spam). Load the data into a dataframe and split it into train and test data set with 20% of the data moved to test set. Preprocess the data, i.e. convert all words to lowercase, remove punctuation, remove stop words, and stem words. Consider using [feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to create the feature vectors. Train a multinomial Naive Bayes classifier on the training data. Evaluate the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data science contest #2\n",
    "\n",
    "Gaussian Naive Bayes is a special case of multinomial Naive Bayes. In Gaussian Naive Bayes, the features are assumed to be continuous values. Specifically, the posterior probability of a class given the features is assumed to be a Gaussian distribution given by:\n",
    "\n",
    "$$P(C_k|F_1, F_2, ..., F_n) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}\\exp\\left(-\\frac{(F_1-\\mu_{k1})^2 + (F_2-\\mu_{k2})^2 + ... + (F_n-\\mu_{kn})^2}{2\\sigma_k^2}\\right)$$\n",
    "\n",
    "where $\\mu_{ki}$ is the mean of the $i$th feature for class $C_k$, and $\\sigma_k$ is the standard deviation of the features for class $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to use the [Iris dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) to train a [Gaussian Naive Bayes classifier](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html). The Iris dataset contains 150 samples of 3 different species of Iris flowers. Each sample has 4 features: sepal length, sepal width, petal length, and petal width. Split the data into train and test sets with 20% of the data moved to the test set. Train a Gaussian Naive Bayes classifier on the training data. Evaluate the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 ('ML_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "929619df38edafcb639067bc1593fc1a1a9b049525cf7e96331131a0abd5060a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
